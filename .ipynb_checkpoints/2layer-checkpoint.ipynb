{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from matplotlib.pylab import (figure, semilogx, loglog, xlabel, ylabel, legend, \n",
    "                           title, subplot, show, grid)\n",
    "\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import model_selection\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import torch\n",
    "from sklearn import model_selection\n",
    "#from toolbox_02450 import train_neural_net, draw_neural_net\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('winequality-white.csv',sep=';')\n",
    "df_plain=pd.read_csv('winequality-white.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict='sulphates'\n",
    "remove_outliers=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________\n",
      "% of data dropped 1.3679052674561043\n"
     ]
    }
   ],
   "source": [
    "shape_orig=df.shape[0]\n",
    "#removal of the 0-1 percentile and 99-100% of the values furthest away from the mean\n",
    "#I will do my prediction of 'sulphates' therefore I do not remove its outliers\n",
    "predict='sulphates'\n",
    "# predict='density'\n",
    "remove_lim=0.001\n",
    "\n",
    "lim_high,lim_low=1-remove_lim,remove_lim\n",
    "\n",
    "if remove_outliers==True:\n",
    "    features=[cols for cols in df.columns if cols != predict]\n",
    "    print(50*'_')\n",
    "    index_names=[]\n",
    "    #FOR all FEATURES get the percentile and add the indexes to a list (index_names)\n",
    "    for feat in features:\n",
    "        y = df[feat]\n",
    "        removed_outliers = y.between(y.quantile(lim_low), y.quantile(lim_high))\n",
    "#         print(feat,'\\n',removed_outliers.value_counts())\n",
    "        for index in list(df[~removed_outliers].index):\n",
    "            if index not in index_names:\n",
    "                index_names.append(index)\n",
    "    #when all are added remove them\n",
    "    df.drop(index_names, inplace=True)\n",
    "    remove_outliers=False\n",
    "print('% of data dropped',100*(1-(df.shape[0]/shape_orig)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outerfold: 0 / 5 innerfold: 0 / 5\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "n_hidden_units: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steve\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:431: UserWarning: Using a target size (torch.Size([3091])) that is different to the input size (torch.Size([3091, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t100\t2.3043551\t0.0034807979\n",
      "new best: 2.3043551\n",
      "n_hidden_units: 2\n",
      "\t\t100\t0.662525\t0.010448874\n",
      "new best: 0.662525\n",
      "n_hidden_units: 3\n",
      "\t\t100\t0.37609747\t0.015282172\n",
      "new best: 0.37609747\n",
      "n_hidden_units: 4\n",
      "\t\t100\t0.03209941\t0.010243973\n",
      "new best: 0.03209941\n",
      "outerfold: 0 / 5 innerfold: 1 / 5\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "n_hidden_units: 1\n",
      "\t\t100\t0.23695962\t0.009827139\n",
      "new best: 0.23695962\n",
      "n_hidden_units: 2\n",
      "\t\t100\t0.2973709\t0.009527606\n",
      "n_hidden_units: 3\n",
      "\t\t100\t0.8216592\t0.007130645\n",
      "n_hidden_units: 4\n",
      "\t\t100\t6.7929196\t0.00796806\n",
      "outerfold: 0 / 5 innerfold: 2 / 5\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "n_hidden_units: 1\n",
      "\t\t100\t0.071741104\t0.012421186\n",
      "new best: 0.071741104\n",
      "n_hidden_units: 2\n",
      "\t\t100\t1.6516491\t0.005636244\n",
      "n_hidden_units: 3\n",
      "\t\t100\t0.044088505\t0.009641315\n",
      "new best: 0.044088505\n",
      "n_hidden_units: 4\n",
      "\t\t100\t0.015818778\t0.002916472\n",
      "new best: 0.015818778\n",
      "outerfold: 0 / 5 innerfold: 3 / 5\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "n_hidden_units: 1\n",
      "\t\t100\t0.012884632\t3.035839e-06\n",
      "new best: 0.012884632\n",
      "n_hidden_units: 2\n",
      "\t\t100\t1.7574861\t0.0063609537\n",
      "n_hidden_units: 3\n",
      "\t\t100\t0.09315535\t0.020010501\n",
      "n_hidden_units: 4\n",
      "\t\t100\t0.44537982\t0.012213626\n",
      "outerfold: 0 / 5 innerfold: 4 / 5\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "n_hidden_units: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steve\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:431: UserWarning: Using a target size (torch.Size([3092])) that is different to the input size (torch.Size([3092, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t100\t0.26333627\t0.0051546507\n",
      "new best: 0.26333627\n",
      "n_hidden_units: 2\n",
      "\t\t100\t0.29028863\t0.007731578\n",
      "n_hidden_units: 3\n",
      "\t\t100\t0.50016683\t0.009482234\n",
      "n_hidden_units: 4\n",
      "\t\t100\t1.7353921\t0.008946326\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-5a61c80e1312>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[1;31m# a good solution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxavier_uniform_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxavier_uniform_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "X=df.drop(columns=predict).values\n",
    "y=df[predict].values\n",
    "attributeNames = [name for name in df.drop(columns=predict)]\n",
    "N, M = X.shape\n",
    "#folds per level\n",
    "K=5\n",
    "\n",
    "#regression defs C1\n",
    "lambdas = np.power(10.,np.linspace(-5,10,num=10))\n",
    "lambdas_opt=np.zeros(K)\n",
    "w_rlr = np.empty((M,K))\n",
    "mu = np.empty((K, M-1))\n",
    "sigma = np.empty((K, M-1))\n",
    "Error_train_rlr = np.empty((K,1))\n",
    "Error_test_rlr = np.empty((K,1))\n",
    "\n",
    "#ANN defs c1\n",
    "n_replicates = 1        # number of networks trained in each k-fold\n",
    "max_iter = 100\n",
    "max_iter_ = 100\n",
    "H = np.array([i for i in range(1,5)])\n",
    "H_opt=np.empty(K)\n",
    "fold_summary=[]\n",
    "loss_fn = torch.nn.MSELoss() # notice how this is now a mean-squared-error loss\n",
    "train_error_ann = np.empty((K,1))\n",
    "test_error_ann = np.empty((K,1))\n",
    "\n",
    "k=0\n",
    "CV1 = model_selection.KFold(K, shuffle=True)\n",
    "for train_index, test_index in CV1.split(X,y):\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    CV2 = model_selection.KFold(K, shuffle=True)\n",
    "    M_ = X_train.shape[1]\n",
    "    \n",
    "    #regression defs c2\n",
    "    M_reg=M+1\n",
    "    w = np.empty((M_reg,K,len(lambdas)))\n",
    "    train_error_reg = np.empty((K,len(lambdas)))\n",
    "    test_error_reg = np.empty((K,len(lambdas)))\n",
    "    \n",
    "    #ANN defs c2\n",
    "    train_error_ann_ = np.empty((K,len(H)))\n",
    "    test_error_ann_ = np.empty((K,len(H)))\n",
    "    \n",
    "    best_final_loss = 1e100\n",
    "    \n",
    "    f = 0\n",
    "    y_train = y_train.squeeze()\n",
    "    \n",
    "    for train_index_, test_index_ in CV2.split(X_train,y_train):\n",
    "        print('outerfold:',k,'/',K,'innerfold:',f,'/',K)\n",
    "        X_train_ = X[train_index_]\n",
    "        y_train_ = y[train_index_]\n",
    "        X_test_ = X[test_index_]\n",
    "        y_test_ = y[test_index_]\n",
    "        \n",
    "        #add w0 for reg\n",
    "        X_train_reg=np.concatenate((np.ones((X_train_.shape[0],1)),X_train_),1)\n",
    "        X_test_reg=np.concatenate((np.ones((X_test_.shape[0],1)),X_test_),1)\n",
    "        \n",
    "        #settings for NN\n",
    "        logging_frequency = 100 # display the loss every 1000th iteration\n",
    "        best_final_loss_ = 1e100\n",
    "        tolerance=1e-6\n",
    "        \n",
    "        #standardize\n",
    "        mu_ = np.mean(X_train_[:, 1:], 0)\n",
    "        sigma_ = np.std(X_train_[:, 1:], 0)\n",
    "        X_train_[:, 1:] = (X_train_[:, 1:] - mu_) / sigma_\n",
    "        X_test_[:, 1:] = (X_test_[:, 1:] - mu_) / sigma_\n",
    "        \n",
    "        #regression\n",
    "        Xty = X_train_reg.T @ y_train_\n",
    "        XtX = X_train_reg.T @ X_train_reg\n",
    "        #run regression model to find optimal lambds C2\n",
    "        for l in range(0,len(lambdas)):\n",
    "            # Compute parameters for current value of lambda and current CV fold\n",
    "            # note: \"linalg.lstsq(a,b)\" is substitue for Matlab's left division operator \"\\\"\n",
    "            lambdaI = lambdas[l] * np.eye(M_reg)\n",
    "            lambdaI[0,0] = 0 # remove bias regularization\n",
    "            w[:,f,l] = np.linalg.solve(XtX+lambdaI,Xty).squeeze()\n",
    "            # Evaluate training and test performance\n",
    "            train_error_reg[f,l] = np.power(y_train_-X_train_reg @ w[:,f,l].T,2).mean(axis=0)\n",
    "            test_error_reg[f,l] = np.power(y_test_-X_test_reg @ w[:,f,l].T,2).mean(axis=0)\n",
    "        \n",
    "        #find optimal h ANN C2\n",
    "        print('\\t\\t{}\\t{}\\t\\t\\t{}'.format('Iter', 'Loss','Rel. loss'))\n",
    "        for count,h in enumerate(H):\n",
    "            print('n_hidden_units:',h)\n",
    "            model_ = lambda: torch.nn.Sequential(\n",
    "                    torch.nn.Linear(M, h), #M features to n_hidden_units\n",
    "                    torch.nn.Tanh(),   # 1st transfer function,\n",
    "                    torch.nn.Linear(h, 1), # n_hidden_units to 1 output neuron\n",
    "                    # no final tranfer function, i.e. \"linear output\"\n",
    "                    )\n",
    "            net_ = model_()\n",
    "        \n",
    "            # initialize weights based on limits that scale with number of in- and\n",
    "            # outputs to the layer, increasing the chance that we converge to \n",
    "            # a good solution\n",
    "            torch.nn.init.xavier_uniform_(net_[0].weight)\n",
    "            torch.nn.init.xavier_uniform_(net_[2].weight)\n",
    "            optimizer = torch.optim.Adam(net_.parameters())\n",
    "            \n",
    "            learning_curve_ = [] # setup storage for loss at each step\n",
    "            old_loss_ = 1e6\n",
    "            \n",
    "            for i in range(max_iter_):\n",
    "                y_est = net_(torch.Tensor(X_train_)) # forward pass, predict labels on training set\n",
    "                y_est_test = net_(torch.Tensor(X_test_))\n",
    "                loss = loss_fn(y_est, torch.Tensor(y_train_)) # determine loss\n",
    "                loss_value_ = loss.data.numpy() #get numpy array instead of tensor\n",
    "                learning_curve_.append(loss_value_) # record loss for later display\n",
    "\n",
    "                # Convergence check, see if the percentual loss decrease is within\n",
    "                # tolerance:\n",
    "                p_delta_loss = np.abs(loss_value_-old_loss_)/old_loss_\n",
    "                if p_delta_loss < tolerance: break\n",
    "                old_loss_ = loss_value_\n",
    "\n",
    "                # display loss with some frequency:\n",
    "                if (i != 0) & ((i+1) % logging_frequency == 0):\n",
    "                    print_str = '\\t\\t' + str(i+1) + '\\t' + str(loss_value_) + '\\t' + str(p_delta_loss)\n",
    "                    print(print_str)\n",
    "                # do backpropagation of loss and optimize weights \n",
    "                optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "            if loss_value_ < best_final_loss_:\n",
    "                best_net_ = net_\n",
    "                best_final_loss_ = loss_value_\n",
    "                best_learning_curve_ = learning_curve_\n",
    "                train_error_ann_[f,count] = np.power(y_train_-y_est.detach().numpy().squeeze(),2).mean(axis=0)\n",
    "                test_error_ann_[f,count] = np.power(y_test_-y_est_test.detach().numpy().squeeze(),2).mean(axis=0)\n",
    "                print('new best:',best_final_loss_)\n",
    "        \n",
    "        \n",
    "        f=f+1\n",
    "    #return optimal lambda\n",
    "    opt_val_err = np.min(np.mean(test_error_reg,axis=0))\n",
    "    opt_lambda = lambdas[np.argmin(np.mean(test_error_reg,axis=0))]\n",
    "    train_err_vs_lambda = np.mean(train_error_reg,axis=0)\n",
    "    test_err_vs_lambda = np.mean(test_error_reg,axis=0)\n",
    "    mean_w_vs_lambda = np.squeeze(np.mean(w,axis=1))\n",
    "    lambdas_opt[k]=opt_lambda\n",
    "    #return optimal h\n",
    "    best_h=H[np.argmin(np.mean(test_error_ann_,axis=0))]\n",
    "    H_opt[k]=best_h\n",
    "    \n",
    "    #baseline\n",
    "    baseline=np.square(y_test-y_test.mean()).sum(axis=0)/y_test.shape[0]\n",
    "    \n",
    "    #regression C1\n",
    "    X_train_reg=np.concatenate((np.ones((X_train.shape[0],1)),X_train),1)\n",
    "    X_test_reg=np.concatenate((np.ones((X_test.shape[0],1)),X_test),1)\n",
    "    mu[k, :] = np.mean(X_train[:, 1:], 0)\n",
    "    sigma[k, :] = np.std(X_train[:, 1:], 0)\n",
    "    X_train[:, 1:] = (X_train[:, 1:] - mu[k, :] ) / sigma[k, :] \n",
    "    X_test[:, 1:] = (X_test[:, 1:] - mu[k, :] ) / sigma[k, :] \n",
    "    Xty = X_train.T @ y_train\n",
    "    XtX = X_train.T @ X_train\n",
    "    \n",
    "    lambdaI = opt_lambda * np.eye(M)\n",
    "    lambdaI[0,0] = 0 # Do no regularize the bias term\n",
    "    w_rlr[:,k] = np.linalg.solve(XtX+lambdaI,Xty).squeeze()\n",
    "    # Compute mean squared error with regularization with optimal lambda\n",
    "    Error_train_rlr[k] = np.square(y_train-X_train @ w_rlr[:,k]).sum(axis=0)/y_train.shape[0]\n",
    "    Error_test_rlr[k] = np.square(y_test-X_test @ w_rlr[:,k]).sum(axis=0)/y_test.shape[0]\n",
    "    \n",
    "    #ANN C1\n",
    "    model_ = lambda: torch.nn.Sequential(\n",
    "        torch.nn.Linear(M, best_h), #M features to n_hidden_units\n",
    "        torch.nn.Tanh(),   # 1st transfer function,\n",
    "        torch.nn.Linear(best_h, 1), # n_hidden_units to 1 output neuron\n",
    "        # no final tranfer function, i.e. \"linear output\"\n",
    "        )\n",
    "    net = model_()\n",
    "\n",
    "    # initialize weights based on limits that scale with number of in- and\n",
    "    # outputs to the layer, increasing the chance that we converge to \n",
    "    # a good solution\n",
    "    torch.nn.init.xavier_uniform_(net[0].weight)\n",
    "    torch.nn.init.xavier_uniform_(net[2].weight)\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "    learning_curve_ = [] # setup storage for loss at each step\n",
    "    old_loss = 1e6\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        y_est = net(torch.Tensor(X_train)) # forward pass, predict labels on training set\n",
    "        y_est_test = net(torch.Tensor(X_test))\n",
    "        loss = loss_fn(y_est, torch.Tensor(y_train)) # determine loss\n",
    "        loss_value = loss.data.numpy() #get numpy array instead of tensor\n",
    "        learning_curve.append(loss_value) # record loss for later display\n",
    "\n",
    "        # Convergence check, see if the percentual loss decrease is within\n",
    "        # tolerance:\n",
    "        p_delta_loss = np.abs(loss_value-old_loss)/old_loss\n",
    "        if p_delta_loss < tolerance: break\n",
    "        old_loss = loss_value\n",
    "\n",
    "        # display loss with some frequency:\n",
    "        if (i != 0) & ((i+1) % logging_frequency == 0):\n",
    "            print_str = '\\t\\t' + str(i+1) + '\\t' + str(loss_value) + '\\t' + str(p_delta_loss)\n",
    "            print(print_str)\n",
    "        # do backpropagation of loss and optimize weights \n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "    if loss_value < best_final_loss:\n",
    "        best_net = net\n",
    "        best_final_loss = loss_value\n",
    "        best_learning_curve = learning_curve\n",
    "        train_error_ann[k] = np.power(y_train-y_est.detach().numpy().squeeze(),2).mean(axis=0)\n",
    "        test_error_ann[k] = np.power(y_test-y_est_test.detach().numpy().squeeze(),2).mean(axis=0)\n",
    "        print('new best:',best_final_loss)\n",
    "    \n",
    "    summary_tags=['fold','h_opt','E_test_ann','lambda_opt','E_test_lrl','baseline']\n",
    "    fold_summary.append([k+1,best_h,np.round(test_error_ann[k][0],3),opt_lambda,np.round(Error_test_rlr[k][0],3),np.round(baseline,3)])\n",
    "    k+=1\n",
    "\n",
    "summary_tags=['fold','h_opt','E_test_ann','lambda_opt','E_test_lrl','baseline']\n",
    "print(summary_tags[0],'\\t',summary_tags[1],'\\t',summary_tags[2],'\\t',summary_tags[3],'\\t',summary_tags[4],'\\t',summary_tags[5])\n",
    "for i in fold_summary:\n",
    "    print(i[0],'\\t',i[1],'\\t',i[2],'\\t\\t',i[3],'\\t\\t',i[4],'\\t\\t',i[5])    \n",
    "#run both models with optimal lambda,h\n",
    "      \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params=dict(net.named_parameters())\n",
    "params\n",
    "\n",
    "# beta = 0.5 #The interpolation parameter    \n",
    "# params1 = model1.named_parameters()\n",
    "# params2 = model2.named_parameters()\n",
    "\n",
    "# dict_params2 = dict(params2)\n",
    "\n",
    "# for name1, param1 in params1:\n",
    "#     if name1 in dict_params2:\n",
    "#         dict_params2[name1].data.copy_(beta*param1.data + (1-beta)*dict_params2[name1].data)\n",
    "\n",
    "# model.load_state_dict(dict_params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
